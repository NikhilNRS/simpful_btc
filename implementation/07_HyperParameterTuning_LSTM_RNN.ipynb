{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77575d7d-00f2-491e-89b1-7d370eb60c11",
   "metadata": {},
   "source": [
    "# Hyper Parameter Tuning using RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d3687e8-5528-4465-8be9-d63dfa542afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Paths to the prepped data files\n",
    "base_path = './data/prepped_data/'  # Adjusted base path to where your files are now\n",
    "X_train_path = base_path + 'X_train.csv'\n",
    "X_test_path = base_path + 'X_test.csv'\n",
    "y_train_path = base_path + 'y_train.csv'\n",
    "y_test_path = base_path + 'y_test.csv'\n",
    "\n",
    "# Loading the datasets\n",
    "X_train = pd.read_csv(X_train_path)\n",
    "X_test = pd.read_csv(X_test_path)\n",
    "y_train = pd.read_csv(y_train_path)\n",
    "y_test = pd.read_csv(y_test_path)\n",
    "\n",
    "# Assuming the last columns of X_train and X_test are year, month, day, hour for reconstructing the datetime index\n",
    "X_train['datetime'] = pd.to_datetime(X_train[['year', 'month', 'day', 'hour']])\n",
    "X_test['datetime'] = pd.to_datetime(X_test[['year', 'month', 'day', 'hour']])\n",
    "y_train.index = pd.to_datetime(X_train['datetime'])\n",
    "y_test.index = pd.to_datetime(X_test['datetime'])\n",
    "\n",
    "# Exclude non-numerical columns before scaling\n",
    "columns_to_scale = X_train.columns.difference(['year', 'month', 'day', 'hour', 'datetime'])\n",
    "\n",
    "# Initialize the scaler for the features\n",
    "scaler_X = MinMaxScaler()\n",
    "\n",
    "# Scale 'X' features (excluding non-numerical columns)\n",
    "X_train_scaled = scaler_X.fit_transform(X_train[columns_to_scale])\n",
    "X_test_scaled = scaler_X.transform(X_test[columns_to_scale])\n",
    "\n",
    "# Initialize a separate scaler for the target variable\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "# Scale 'y' (the target variable)\n",
    "y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1)).flatten()\n",
    "y_test_scaled = scaler_y.transform(y_test.values.reshape(-1, 1)).flatten()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7881ee7e-41ef-4b99-b2b0-fb1f0f4c32be",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3696f698-79ae-4842-ae26-acb928d93a74",
   "metadata": {},
   "source": [
    "## Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be48824-1f75-4a05-b386-3f70d66a5185",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikhilrazab-sekh/Desktop/simpful_btc/implementation/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "[I 2024-03-18 22:59:25,536] A new study created in memory with name: no-name-caaa2c31-beeb-4d87-a13e-15a8dae8c805\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c9fdbba806d4051921de318449e6cd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikhilrazab-sekh/Desktop/simpful_btc/implementation/.venv/lib/python3.9/site-packages/keras/src/layers/rnn/rnn.py:205: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "/Users/nikhilrazab-sekh/Desktop/simpful_btc/implementation/.venv/lib/python3.9/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 23:04:50,981] Trial 0 finished with value: 0.008750918321311474 and parameters: {'units': 50, 'activation': 'relu', 'learning_rate': 0.00018385008016090357, 'batch_size': 32}. Best is trial 0 with value: 0.008750918321311474.\n",
      "[I 2024-03-18 23:09:06,439] Trial 1 finished with value: 0.00778465811163187 and parameters: {'units': 50, 'activation': 'relu', 'learning_rate': 0.0024869512390378475, 'batch_size': 64}. Best is trial 1 with value: 0.00778465811163187.\n",
      "[I 2024-03-18 23:13:25,929] Trial 2 finished with value: 0.0033894293010234833 and parameters: {'units': 50, 'activation': 'relu', 'learning_rate': 0.0036323905906861884, 'batch_size': 64}. Best is trial 2 with value: 0.0033894293010234833.\n",
      "[I 2024-03-18 23:28:11,075] Trial 3 finished with value: 0.005231366027146578 and parameters: {'units': 150, 'activation': 'tanh', 'learning_rate': 0.002739119041734605, 'batch_size': 32}. Best is trial 2 with value: 0.0033894293010234833.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import optuna\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Assuming the following variables are defined and properly prepared:\n",
    "# X_train_scaled, y_train_scaled, X_test_scaled, y_test_scaled, scaler_y\n",
    "\n",
    "n_input = 168  # Number of time steps to look back for predictions\n",
    "n_features = X_train_scaled.shape[1]  # Number of features in the dataset\n",
    "\n",
    "def objective(trial):\n",
    "    # Hyperparameters to be tuned\n",
    "    units = trial.suggest_categorical('units', [50, 100, 150])\n",
    "    activation = trial.suggest_categorical('activation', ['relu', 'tanh'])\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n",
    "    \n",
    "    # Model definition\n",
    "    model = Sequential([\n",
    "        LSTM(units=units, activation=activation, input_shape=(n_input, n_features)),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')\n",
    "    \n",
    "    # Fit model using TimeseriesGenerator\n",
    "    train_generator = TimeseriesGenerator(X_train_scaled, y_train_scaled, length=n_input, batch_size=batch_size)\n",
    "    validation_generator = TimeseriesGenerator(X_test_scaled, y_test_scaled, length=n_input, batch_size=batch_size)\n",
    "    model.fit(train_generator, epochs=10, validation_data=validation_generator, verbose=0)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    val_loss = model.evaluate(validation_generator, verbose=0)\n",
    "    return val_loss\n",
    "\n",
    "# Optimize\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=10, show_progress_bar=True)\n",
    "\n",
    "# After optimization, print best parameters\n",
    "best_params = study.best_trial.params\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "\n",
    "# Correct approach to use the best parameters to create a new model\n",
    "model = Sequential([\n",
    "    LSTM(units=best_params['units'], activation=best_params['activation'], input_shape=(n_input, n_features)),\n",
    "    Dense(1)\n",
    "])\n",
    "model.compile(optimizer=Adam(learning_rate=best_params['learning_rate']), loss='mse')\n",
    "\n",
    "# It's suggested to retrain your model here with the entire dataset or a combined train-validation set if applicable\n",
    "# Adjust batch_size based on best_params if it was part of the optimization\n",
    "train_generator = TimeseriesGenerator(X_train_scaled, y_train_scaled, length=n_input, batch_size=best_params['batch_size'])\n",
    "model.fit(train_generator, epochs=10, verbose=1)  # Or more epochs as needed\n",
    "\n",
    "# Final evaluation, potentially on a separate test set if you have one\n",
    "validation_generator = TimeseriesGenerator(X_test_scaled, y_test_scaled, length=n_input, batch_size=best_params['batch_size'])\n",
    "y_pred_scaled = model.predict(validation_generator)\n",
    "y_test_original = y_test.values.flatten()[:len(y_pred)]  # Adjust length if necessary\n",
    "y_pred = scaler_y.inverse_transform(y_pred_scaled)\n",
    "\n",
    "# Calculate RMSE and MAPE\n",
    "rmse = math.sqrt(mean_squared_error(y_test_original, y_pred))\n",
    "mape = np.mean(np.abs((y_test_original - y_pred) / y_test_original)) * 100\n",
    "\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"MAPE: {mape}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ca4b47-f852-4896-b3e9-3310fca9e299",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "y_test_original = y_test.values.flatten()[:len(y_pred)]  # Adjust length if necessary\n",
    "# Assuming y_test_original and y_pred are correctly aligned with these dates\n",
    "test_dates_aligned = X_test['datetime'].values[-len(y_pred):]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(test_dates_aligned, y_test_original, label='Actual', marker='.', linestyle='-', linewidth=1)\n",
    "plt.plot(test_dates_aligned, y_pred, label='Predicted', alpha=0.7, marker='.', linestyle='--', linewidth=1)\n",
    "plt.title('Actual vs Predicted')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "\n",
    "# Improve readability of the date labels\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1d6233-2073-44b2-be20-2a01449df07b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
