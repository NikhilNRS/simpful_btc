{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77575d7d-00f2-491e-89b1-7d370eb60c11",
   "metadata": {},
   "source": [
    "# Hyper Parameter Tuning using RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d3687e8-5528-4465-8be9-d63dfa542afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Paths to the prepped data files\n",
    "base_path = './data/prepped_data/'  # Adjusted base path to where your files are now\n",
    "X_train_path = base_path + 'X_train.csv'\n",
    "X_test_path = base_path + 'X_test.csv'\n",
    "y_train_path = base_path + 'y_train.csv'\n",
    "y_test_path = base_path + 'y_test.csv'\n",
    "\n",
    "# Loading the datasets\n",
    "X_train = pd.read_csv(X_train_path)\n",
    "X_test = pd.read_csv(X_test_path)\n",
    "y_train = pd.read_csv(y_train_path)\n",
    "y_test = pd.read_csv(y_test_path)\n",
    "\n",
    "# Assuming the last columns of X_train and X_test are year, month, day, hour for reconstructing the datetime index\n",
    "X_train['datetime'] = pd.to_datetime(X_train[['year', 'month', 'day', 'hour']])\n",
    "X_test['datetime'] = pd.to_datetime(X_test[['year', 'month', 'day', 'hour']])\n",
    "y_train.index = pd.to_datetime(X_train['datetime'])\n",
    "y_test.index = pd.to_datetime(X_test['datetime'])\n",
    "\n",
    "# Exclude non-numerical columns before scaling\n",
    "columns_to_scale = X_train.columns.difference(['year', 'month', 'day', 'hour', 'datetime'])\n",
    "\n",
    "# Initialize the scaler for the features\n",
    "scaler_X = MinMaxScaler()\n",
    "\n",
    "# Scale 'X' features (excluding non-numerical columns)\n",
    "X_train_scaled = scaler_X.fit_transform(X_train[columns_to_scale])\n",
    "X_test_scaled = scaler_X.transform(X_test[columns_to_scale])\n",
    "\n",
    "# Initialize a separate scaler for the target variable\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "# Scale 'y' (the target variable)\n",
    "y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1)).flatten()\n",
    "y_test_scaled = scaler_y.transform(y_test.values.reshape(-1, 1)).flatten()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7881ee7e-41ef-4b99-b2b0-fb1f0f4c32be",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3696f698-79ae-4842-ae26-acb928d93a74",
   "metadata": {},
   "source": [
    "## Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2be48824-1f75-4a05-b386-3f70d66a5185",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikhilrazab-sekh/Desktop/simpful_btc/implementation/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "[I 2024-03-18 23:42:42,390] A new study created in memory with name: no-name-4cb005ab-0ed4-452e-b98f-a5ea757d0c05\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a89dcdcaf0d7499cbc5335a615c12d8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikhilrazab-sekh/Desktop/simpful_btc/implementation/.venv/lib/python3.9/site-packages/keras/src/layers/rnn/rnn.py:205: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "/Users/nikhilrazab-sekh/Desktop/simpful_btc/implementation/.venv/lib/python3.9/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 23:52:40,042] Trial 0 finished with value: 0.0016602873802185059 and parameters: {'units': 150, 'activation': 'tanh', 'learning_rate': 0.0005699401056155138, 'batch_size': 128}. Best is trial 0 with value: 0.0016602873802185059.\n",
      "[I 2024-03-19 00:04:35,491] Trial 1 finished with value: 0.00046483927872031927 and parameters: {'units': 100, 'activation': 'tanh', 'learning_rate': 0.00015051111249499415, 'batch_size': 32}. Best is trial 1 with value: 0.00046483927872031927.\n",
      "[I 2024-03-19 00:17:23,549] Trial 2 finished with value: 0.0038473824970424175 and parameters: {'units': 150, 'activation': 'relu', 'learning_rate': 0.00020924432249771975, 'batch_size': 64}. Best is trial 1 with value: 0.00046483927872031927.\n",
      "[I 2024-03-19 00:21:28,236] Trial 3 finished with value: 0.0014548702165484428 and parameters: {'units': 50, 'activation': 'relu', 'learning_rate': 0.00011872476903671935, 'batch_size': 64}. Best is trial 1 with value: 0.00046483927872031927.\n",
      "[I 2024-03-19 00:28:00,052] Trial 4 finished with value: 0.008504552766680717 and parameters: {'units': 100, 'activation': 'tanh', 'learning_rate': 0.00809362861136003, 'batch_size': 128}. Best is trial 1 with value: 0.00046483927872031927.\n",
      "[I 2024-03-19 00:36:39,389] Trial 5 finished with value: 0.006539441179484129 and parameters: {'units': 100, 'activation': 'relu', 'learning_rate': 0.0011016518060755595, 'batch_size': 64}. Best is trial 1 with value: 0.00046483927872031927.\n",
      "[I 2024-03-19 00:50:09,462] Trial 6 finished with value: 0.006091439165174961 and parameters: {'units': 150, 'activation': 'tanh', 'learning_rate': 0.00014877054742193358, 'batch_size': 64}. Best is trial 1 with value: 0.00046483927872031927.\n",
      "[I 2024-03-19 00:53:59,722] Trial 7 finished with value: 0.02498544752597809 and parameters: {'units': 50, 'activation': 'relu', 'learning_rate': 0.00012695230930418266, 'batch_size': 128}. Best is trial 1 with value: 0.00046483927872031927.\n",
      "[I 2024-03-19 01:09:55,752] Trial 8 finished with value: 0.004617282655090094 and parameters: {'units': 150, 'activation': 'tanh', 'learning_rate': 0.008157791169835603, 'batch_size': 32}. Best is trial 1 with value: 0.00046483927872031927.\n",
      "[I 2024-03-19 01:14:12,045] Trial 9 finished with value: 0.002192156622186303 and parameters: {'units': 50, 'activation': 'tanh', 'learning_rate': 0.002231934681845389, 'batch_size': 64}. Best is trial 1 with value: 0.00046483927872031927.\n",
      "Best parameters: {'units': 100, 'activation': 'tanh', 'learning_rate': 0.00015051111249499415, 'batch_size': 32}\n",
      "Epoch 1/10\n",
      "\u001b[1m1769/1769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 34ms/step - loss: 0.0376\n",
      "Epoch 2/10\n",
      "\u001b[1m1769/1769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 34ms/step - loss: 8.0036e-04\n",
      "Epoch 3/10\n",
      "\u001b[1m1769/1769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 33ms/step - loss: 4.9066e-04\n",
      "Epoch 4/10\n",
      "\u001b[1m1769/1769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 33ms/step - loss: 3.6826e-04\n",
      "Epoch 5/10\n",
      "\u001b[1m1769/1769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 34ms/step - loss: 2.9797e-04\n",
      "Epoch 6/10\n",
      "\u001b[1m1769/1769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 34ms/step - loss: 2.3442e-04\n",
      "Epoch 7/10\n",
      "\u001b[1m1769/1769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 33ms/step - loss: 2.2191e-04\n",
      "Epoch 8/10\n",
      "\u001b[1m1769/1769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 33ms/step - loss: 1.0926e-04\n",
      "Epoch 9/10\n",
      "\u001b[1m1769/1769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 33ms/step - loss: 1.1968e-04\n",
      "Epoch 10/10\n",
      "\u001b[1m1769/1769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 33ms/step - loss: 9.6565e-05\n",
      "\u001b[1m439/439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'y_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 62\u001b[0m\n\u001b[1;32m     60\u001b[0m validation_generator \u001b[38;5;241m=\u001b[39m TimeseriesGenerator(X_test_scaled, y_test_scaled, length\u001b[38;5;241m=\u001b[39mn_input, batch_size\u001b[38;5;241m=\u001b[39mbest_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     61\u001b[0m y_pred_scaled \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(validation_generator)\n\u001b[0;32m---> 62\u001b[0m y_test_original \u001b[38;5;241m=\u001b[39m y_test\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mflatten()[:\u001b[38;5;28mlen\u001b[39m(\u001b[43my_pred\u001b[49m)]  \u001b[38;5;66;03m# Adjust length if necessary\u001b[39;00m\n\u001b[1;32m     63\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m scaler_y\u001b[38;5;241m.\u001b[39minverse_transform(y_pred_scaled)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Calculate RMSE and MAPE\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_pred' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import optuna\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Assuming the following variables are defined and properly prepared:\n",
    "# X_train_scaled, y_train_scaled, X_test_scaled, y_test_scaled, scaler_y\n",
    "\n",
    "n_input = 168  # Number of time steps to look back for predictions\n",
    "n_features = X_train_scaled.shape[1]  # Number of features in the dataset\n",
    "\n",
    "def objective(trial):\n",
    "    # Hyperparameters to be tuned\n",
    "    units = trial.suggest_categorical('units', [50, 100, 150])\n",
    "    activation = trial.suggest_categorical('activation', ['relu', 'tanh'])\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n",
    "    \n",
    "    # Model definition\n",
    "    model = Sequential([\n",
    "        LSTM(units=units, activation=activation, input_shape=(n_input, n_features)),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')\n",
    "    \n",
    "    # Fit model using TimeseriesGenerator\n",
    "    train_generator = TimeseriesGenerator(X_train_scaled, y_train_scaled, length=n_input, batch_size=batch_size)\n",
    "    validation_generator = TimeseriesGenerator(X_test_scaled, y_test_scaled, length=n_input, batch_size=batch_size)\n",
    "    model.fit(train_generator, epochs=10, validation_data=validation_generator, verbose=0)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    val_loss = model.evaluate(validation_generator, verbose=0)\n",
    "    return val_loss\n",
    "\n",
    "# Optimize\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=10, show_progress_bar=True)\n",
    "\n",
    "# After optimization, print best parameters\n",
    "best_params = study.best_trial.params\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "\n",
    "# Correct approach to use the best parameters to create a new model\n",
    "model = Sequential([\n",
    "    LSTM(units=best_params['units'], activation=best_params['activation'], input_shape=(n_input, n_features)),\n",
    "    Dense(1)\n",
    "])\n",
    "model.compile(optimizer=Adam(learning_rate=best_params['learning_rate']), loss='mse')\n",
    "\n",
    "# It's suggested to retrain your model here with the entire dataset or a combined train-validation set if applicable\n",
    "# Adjust batch_size based on best_params if it was part of the optimization\n",
    "train_generator = TimeseriesGenerator(X_train_scaled, y_train_scaled, length=n_input, batch_size=best_params['batch_size'])\n",
    "model.fit(train_generator, epochs=10, verbose=1)  # Or more epochs as needed\n",
    "\n",
    "# Final evaluation, potentially on a separate test set if you have one\n",
    "validation_generator = TimeseriesGenerator(X_test_scaled, y_test_scaled, length=n_input, batch_size=best_params['batch_size'])\n",
    "y_pred_scaled = model.predict(validation_generator)\n",
    "y_test_original = y_test.values.flatten()[:len(y_pred)]  # Adjust length if necessary\n",
    "y_pred = scaler_y.inverse_transform(y_pred_scaled)\n",
    "\n",
    "# Calculate RMSE and MAPE\n",
    "rmse = math.sqrt(mean_squared_error(y_test_original, y_pred))\n",
    "mape = np.mean(np.abs((y_test_original - y_pred) / y_test_original)) * 100\n",
    "\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"MAPE: {mape}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ca4b47-f852-4896-b3e9-3310fca9e299",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "y_test_original = y_test.values.flatten()[:len(y_pred)]  # Adjust length if necessary\n",
    "# Assuming y_test_original and y_pred are correctly aligned with these dates\n",
    "test_dates_aligned = X_test['datetime'].values[-len(y_pred):]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(test_dates_aligned, y_test_original, label='Actual', marker='.', linestyle='-', linewidth=1)\n",
    "plt.plot(test_dates_aligned, y_pred, label='Predicted', alpha=0.7, marker='.', linestyle='--', linewidth=1)\n",
    "plt.title('Actual vs Predicted')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "\n",
    "# Improve readability of the date labels\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1d6233-2073-44b2-be20-2a01449df07b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
